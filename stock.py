# -*- coding: utf-8 -*-
"""stock.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e0GHu2B66oW8jTyGgHY6rP3eB_2OC0-Z
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from scipy.stats import skew
from google.colab import drive
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

#Import Training Dataset.
data =  pd.read_csv("/Train.csv")
print(data)

data.head(10) #Top 10 entries
data.tail(10) #Bottom 10 entries

# Details about dataset attributes.
data.describe()

#Random Selection of dataset to see distribution of values
data.sample(10)

#Dimension of dataset
data.shape
# Information about data types.
data.info()

#check for NA Values.
pd.isnull(data)
data.isnull().sum()

data.boxplot(column = ["Quantity"])
plt.show()

data2 = data[(data.Quantity<20000) & (data.Quantity>-20000)]
data2

data2.boxplot(column="Quantity")
plt.show()

data2.boxplot(column = "Quantity")
plt.show()
data3 = data2[data2.Quantity<2000]
data3.boxplot(column = "Quantity")
plt.show()

data3.shape 
skew(data3.Quantity) #Skew = 15.3
skew(np.sin(data3.Quantity)) #Skew = 0.13 
skew(np.cos(data3.Quantity)) #Skew = -0.29

plt.boxplot(data3.Quantity)
plt.show()
plt.boxplot(np.sin(data3.Quantity))
plt.show()
plt.boxplot(np.cos(data3.Quantity))
plt.show()

skew(data3.Description)
data3.boxplot(column= ["Description"])
plt.show

data3.boxplot(column = ["UnitPrice"])
plt.show()

data4 = data3[data3.UnitPrice<2000]
data4.boxplot(column = "UnitPrice")
plt.show()

skew(data4.UnitPrice) #76.44
skew(np.log(data4.UnitPrice+1)) #0.99
skew(np.sin(data4.UnitPrice)) #-1.10
skew(np.cos(data4.UnitPrice)) #-0.04

plt.boxplot(data4.UnitPrice)
plt.show()
plt.boxplot(np.log(data.UnitPrice+1))
plt.show()
plt.boxplot(np.sin(data.UnitPrice))
plt.show()
plt.boxplot(np.cos(data.UnitPrice))
plt.show()

data4.shape

# Finding the relations between the variables.
plt.figure(figsize=(20,10))
c= data4.corr()
sns.heatmap(c,cmap="YlGnBu",annot=True)
c

data4['Country'] = data4['Country'].astype('category')
data4.Country.unique()

a4_dims = (11.7, 8.27)
fig, ax = plt.subplots(figsize=a4_dims)
sns.boxenplot(ax =ax,x="Country", y="UnitPrice", data=data4)

data4.groupby("Country").UnitPrice.mean().sort_values(ascending=False)[:37].plot.bar()

sns.pairplot(data4)

Q1 = data4.quantile(0.25)
Q3 = data4.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

#calculate the percentage of each UnitPrice category.
data.UnitPrice.value_counts(normalize=True)

#plot the pie chart of UnitPrice categories
data.UnitPrice.value_counts(normalize=True).plot.pie()
plt.show()

#Drop the date column
data5 = data4.drop(['InvoiceDate'],axis = 1)
data5

#consider those stocks only whose unit price is greater than 0
data6 = data5[data5.UnitPrice>0]
data6

#Consider UnitPrice as our target variable.
labels = np.array(data6['UnitPrice'])

#Drop the UnitPrice column.
data7 = data6.drop(["UnitPrice"],axis =1)
data7

data_list = list(data7.columns)
data8 = np.array(data7)

# Using Skicit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split
# Split the data into training and testing sets
train_features, test_features, train_labels, test_labels = train_test_split(data8, labels, test_size = 0.25, random_state = 42)

print('Training Features Shape:', train_features.shape)
print('Training Labels Shape:', train_labels.shape)
print('Testing Features Shape:', test_features.shape)
print('Testing Labels Shape:', test_labels.shape)

# Import the Random Forest Regressor model
from sklearn.ensemble import RandomForestRegressor
# Instantiate model with 1000 decision trees
rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)
# Train the model on training data
rf.fit(train_features, train_labels);

# Use the forest's predict method on the test data
predictions = rf.predict(test_features)
# Calculate the absolute errors
errors = abs(predictions - test_labels)
# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')

from sklearn.metrics import mean_squared_error
import math

mse = mean_squared_error(test_labels, predictions)
rmse = math.sqrt(mse)
print(rmse)

# Calculate mean absolute percentage error (MAPE)
mape = 100 * (errors / test_labels)
# Calculate and display accuracy
accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')

#Import Training Dataset.
Test_dataset =  pd.read_csv("/content/Test.csv")
print(Test_dataset)

Test_dataset.columns

Test_dataset2 = Test_dataset.drop(columns="InvoiceDate")
Test_dataset2.columns

Test_dataset2['Country'] = Test_dataset2['Country'].astype('category')
Test_dataset2.Country.unique()

#check for NA Values.
pd.isnull(Test_dataset2)
Test_dataset2.isnull().sum()

# Use the forest's predict method on the test dataset
predictions2 = rf.predict(Test_dataset2)
predictions2

result = pd.DataFrame(predictions2)

result.to_csv('file1.csv')

import csv
with open('predictions2.csv', 'w', newline='') as file:
    writer = csv.writer(file, delimiter='|')
    writer.writerows(predictions2.csv)

np.savetxt('data.csv',predictions2)